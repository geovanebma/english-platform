// import 'dotenv/config';
// import { GoogleGenAI } from '@google/genai';

// const MAX_RETRIES = 3;
// const DELAY_TIME = 2000;
// const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
// const apiKey = process.env.GEMINI_API_KEY;

// if (!apiKey) {
//     console.error("ERRO CR√çTICO: A chave GEMINI_API_KEY n√£o foi encontrada. Verifique o arquivo .env.");
//     process.exit(1);
// }

// const ai = new GoogleGenAI({ apiKey });

// async function gerarTexto(prompt) {
//     console.log(`\nü§ñ PROCESSANDO PROMPT: "${prompt}"`);

//     for (let i = 0; i < MAX_RETRIES; i++) {
//         try {
//             const response = await ai.models.generateContent({
//                 model: "gemini-2.5-flash",
//                 contents: [{ role: "user", parts: [{ text: prompt }] }],
//                 config: {
//                     temperature: 0.7,
//                 }
//             });

//             const textoGerado = response.text;

//             console.log("--- ‚úÖ SUCESSO | Resposta da IA ---");
//             console.log(textoGerado);
//             console.log("----------------------------------\n");
//             return;

//         } catch (error) {
//             const errorMessage = error.message;
//             const isOverloadedError = errorMessage.includes("503") || errorMessage.includes("UNAVAILABLE");

//             if (isOverloadedError && i < MAX_RETRIES - 1) {
//                 const currentWaitTime = DELAY_TIME * (i + 1);

//                 console.log(`‚ö†Ô∏è Erro tempor√°rio (503/Sobrecarga). Tentativa ${i + 1}/${MAX_RETRIES}.`);
//                 console.log(`   Reexecutando em ${currentWaitTime / 1000} segundos...`);

//                 await delay(currentWaitTime);

//             } else {
//                 console.error(`‚ùå ERRO FATAL: Falha ao gerar texto ap√≥s ${i + 1} tentativas.`);
//                 console.error("   Detalhes do erro:", errorMessage);
//                 break;
//             }
//         }
//     }
// }

// async function main() {
//     console.log("Iniciando o gerador de texto com IA...");

//     await gerarTexto("Crie um insert into assim: INSERT INTO listening_lessons VALUES ('A1', title 'Greetings', text 'um texto envolvendo o t√≠tulo gramatical informado no title', translation 'tradu√ß√£o do texto em portugu√™s', difficulty '1 a 5', type 'DIALOGUE, DICTATION...')");

//     console.log("\nExecu√ß√£o de todos os prompts finalizada.");
// }

// main();

import 'dotenv/config';
import express from 'express';
import { GoogleGenAI } from '@google/genai';

// --- Configura√ß√£o de Retentativa ---
const MAX_RETRIES = 3;
const DELAY_TIME = 2000;
const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
// -----------------------------------

const apiKey = process.env.GEMINI_API_KEY;

// Instru√ß√£o do Sistema para definir o papel da IA (Tutor de Ingl√™s)
const SYSTEM_INSTRUCTION = "Voc√™ √© um parceiro de conversa√ß√£o em ingl√™s chamado 'TutorAI'. Seu objetivo √© ajudar o usu√°rio a praticar ingl√™s. Responda de forma natural em ingl√™s, mantendo a conversa fluindo. Se o usu√°rio cometer um erro gramatical ou de vocabul√°rio, forne√ßa a corre√ß√£o sutilmente na sua resposta e, no final, adicione uma se√ß√£o separada em portugu√™s (iniciando com '‚ú® Corre√ß√£o:') explicando o erro e a corre√ß√£o de forma amig√°vel e encorajadora.";

if (!apiKey) {
    console.error("ERRO CR√çTICO: A chave GEMINI_API_KEY n√£o foi encontrada. Verifique o arquivo .env.");
    process.exit(1);
}

const ai = new GoogleGenAI({ apiKey });
const app = express();
const PORT = process.env.PORT || 3000;

// Middleware para permitir JSON no corpo da requisi√ß√£o
app.use(express.json());

// Middleware CORS b√°sico para permitir que o frontend React se conecte (ajustar para produ√ß√£o)
app.use((req, res, next) => {
    res.setHeader('Access-Control-Allow-Origin', '*'); // Permite qualquer origem
    res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
    res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
    if (req.method === 'OPTIONS') {
        return res.sendStatus(200);
    }
    next();
});

/**
 * Endpoint para processar a conversa com a IA.
 * Recebe o hist√≥rico completo (contents) e envia para o modelo Gemini.
 * Rota: POST /api/chat
 * Corpo: { contents: Array<Part> }
 */
app.post('/api/chat', async (req, res) => {
    const { contents } = req.body;

    if (!contents || !Array.isArray(contents)) {
        return res.status(400).json({ error: "O corpo da requisi√ß√£o deve conter 'contents' como um array de mensagens (hist√≥rico do chat)." });
    }

    console.log(`\nü§ñ PROCESSANDO CHAT COM ${contents.length} MENSAGENS...`);

    for (let i = 0; i < MAX_RETRIES; i++) {
        try {
            const response = await ai.models.generateContent({
                model: "gemini-2.5-flash",
                // Passa o hist√≥rico completo do chat
                contents: contents, 
                config: {
                    temperature: 0.8, // Mais criativo para conversa√ß√£o
                },
                systemInstruction: {
                    parts: [{ text: SYSTEM_INSTRUCTION }]
                }
            });

            const textoGerado = response.text;
            console.log("--- ‚úÖ SUCESSO | Resposta da IA no Chat ---\n", textoGerado);

            // Retorna a resposta da IA para o frontend
            return res.json({ responseText: textoGerado });

        } catch (error) {
            const errorMessage = error.message;
            const isOverloadedError = errorMessage.includes("503") || errorMessage.includes("UNAVAILABLE");

            if (isOverloadedError && i < MAX_RETRIES - 1) {
                const currentWaitTime = DELAY_TIME * (i + 1);
                console.log(`‚ö†Ô∏è Erro tempor√°rio (503/Sobrecarga). Tentativa ${i + 1}/${MAX_RETRIES}. Reexecutando em ${currentWaitTime / 1000}s...`);
                await delay(currentWaitTime);
            } else {
                console.error(`‚ùå ERRO FATAL: Falha ao gerar texto ap√≥s ${i + 1} tentativas.`);
                console.error("   Detalhes do erro:", errorMessage);
                
                // Retorna um erro 500 para o frontend
                return res.status(500).json({ error: "Falha ao processar a mensagem no servidor.", details: errorMessage });
            }
        }
    }
    
    // Se sair do loop de retries sem sucesso
    return res.status(500).json({ error: "Falha ao processar a mensagem ap√≥s v√°rias tentativas." });
});

// Inicializa o servidor
app.listen(PORT, () => {
    console.log(`Servidor rodando na porta ${PORT}`);
    console.log(`Endpoint de Chat: http://localhost:${PORT}/api/chat`);
});